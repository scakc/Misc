{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network implementation\n",
    "\n",
    "This notebook shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for tensorflow, but you will find it easy to adapt it to almost any python-based deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play some old videogames\n",
    "![img](https://s17.postimg.org/y9xcab74f/nerd.png)\n",
    "\n",
    "This time we're gonna apply approximate q-learning to an atari game called Breakout. It's not the hardest thing out there, but it's definitely way more complex than anything we tried before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing game image \n",
    "\n",
    "Raw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n",
    "\n",
    "We can thus save a lot of time by preprocessing game image, including\n",
    "* Resizing to a smaller shape, 64 x 64\n",
    "* Converting to grayscale\n",
    "* Cropping irrelevant image parts (top & bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "from scipy.misc import imresize\n",
    "\n",
    "class PreprocessAtari(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self,env)\n",
    "        \n",
    "        self.img_size = (64, 64)\n",
    "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
    "\n",
    "    def _observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "        \n",
    "        # Here's what you need to do:\n",
    "        #  * crop image, remove irrelevant parts\n",
    "        img = img[34:-16, :, :]\n",
    "        #  * resize image to self.img_size \n",
    "        img = imresize(img, self.img_size)\n",
    "        #     (use imresize imported above or any library you want,\n",
    "        #      e.g. opencv, skimage, PIL, keras)\n",
    "        #  * cast image to grayscale\n",
    "        img = img.mean(-1, keepdims=True)\n",
    "        #  * convert image pixels to (0,1) range, float32 type\n",
    "        img = img.astype('float32') / 255.0\n",
    "        \n",
    "        \n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: <class '__main__.PreprocessAtari'> doesn't implement 'observation' method. Maybe it implements deprecated '_observation' method.\u001b[0m\n",
      "Formal tests seem fine. Here's an example of what you'll get.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "#spawn game instance for tests\n",
    "env = gym.make(\"BreakoutDeterministic-v0\") #create raw env\n",
    "env = PreprocessAtari(env)\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "#test observation\n",
    "assert obs.ndim == 3, \"observation must be [batch, time, channels] even if there's just one channel\"\n",
    "assert obs.shape == observation_shape\n",
    "assert obs.dtype == 'float32'\n",
    "assert len(np.unique(obs))>2, \"your image must not be binary\"\n",
    "assert 0 <= np.min(obs) and np.max(obs) <=1, \"convert image pixels to (0,1) range\"\n",
    "\n",
    "print(\"Formal tests seem fine. Here's an example of what you'll get.\")\n",
    "\n",
    "plt.title(\"what your network gonna see\")\n",
    "plt.imshow(obs[:,:,0],interpolation='none',cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame buffer\n",
    "\n",
    "Our agent can only process one observation at a time, so we gotta make sure it contains enough information to fing optimal actions. For instance, agent has to react to moving objects so he must be able to measure object's velocity.\n",
    "\n",
    "To do so, we introduce a buffer that stores 4 last images. This time everything is pre-implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    }
   ],
   "source": [
    "from framebuffer import FrameBuffer\n",
    "def make_env():\n",
    "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "    env = PreprocessAtari(env)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    obs, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "\n",
    "plt.title(\"Game image\")\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "plt.show()\n",
    "plt.title(\"Agent observation (4 frames left to right)\")\n",
    "plt.imshow(obs.transpose([0,2,1]).reshape([state_dim[0],-1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n",
    "\n",
    "You can build any architecture you want, but for reference, here's something that will more or less work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s17.postimg.org/ogg4xo51r/dqn_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "import keras\n",
    "class DQNAgent:\n",
    "    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            \n",
    "            # < Define your network body here. Please make sure you don't use any layers created elsewhere >\n",
    "            self.network = keras.models.Sequential()\n",
    "    \n",
    "            # Keras ignores the first dimension in the input_shape, which is the batch size. \n",
    "            # So just use state_shape for the input shape\n",
    "            self.network.add(Conv2D(16, (3, 3), strides=2, activation='relu', input_shape=state_shape))\n",
    "            self.network.add(Conv2D(32, (3, 3), strides=2, activation='relu'))\n",
    "            self.network.add(Conv2D(64, (3, 3), strides=2, activation='relu'))\n",
    "            self.network.add(Flatten())\n",
    "            self.network.add(Dense(256, activation='relu'))\n",
    "            self.network.add(Dense(n_actions, activation='linear'))\n",
    "            \n",
    "            # prepare a graph for agent step\n",
    "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
    "            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n",
    "            \n",
    "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_symbolic_qvalues(self, state_t):\n",
    "        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n",
    "        \n",
    "        qvalues = self.network(state_t)\n",
    "        \n",
    "        \n",
    "        assert tf.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n",
    "            \"please return 2d tf tensor of qvalues [you got %s]\" % repr(qvalues)\n",
    "        assert int(qvalues.shape[1]) == n_actions\n",
    "        \n",
    "        return qvalues\n",
    "    \n",
    "    def get_qvalues(self, state_t):\n",
    "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
    "    \n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abhikcr/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(\"dqn_agent\", state_dim, n_actions, epsilon=0.5)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "agent.network = keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done: break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, agent, n_games=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here __to get 2 bonus points__.\n",
    "\n",
    "![img](https://s17.postimg.org/ms4zvqj4v/exp_replay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    }
   ],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset(), env.action_space.sample(), 1.0, env.reset(), done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    :returns: return sum of rewards over time\n",
    "    \n",
    "    Note: please do not env.reset() unless env is done.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "    \"\"\"\n",
    "    # State at the beginning of rollout\n",
    "    s = env.framebuffer\n",
    "    \n",
    "    # Play the game for n_steps as per instructions above\n",
    "    reward = 0.0\n",
    "    for t in range(n_steps):\n",
    "        # get agent to pick action given state s\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        next_s, r, done, _ = env.step(action)\n",
    "        \n",
    "        # add to replay buffer\n",
    "        exp_replay.add(s, action, r, next_s, done)\n",
    "        reward += r\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    }
   ],
   "source": [
    "# testing your code. This may take a minute...\n",
    "exp_replay = ReplayBuffer(1)\n",
    "\n",
    "play_and_record(agent, env, exp_replay, n_steps=1)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction. \n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1, \"play_and_record should have added exactly 10000 steps, \"\\\n",
    "                                 \"but instead added %i\"%len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "# assert 0 < np.mean(is_dones) < 0.1, \"Please make sure you restart the game whenever it is 'done' and record the is_done correctly into the buffer.\"\\\n",
    "#                                     \"Got %f is_done rate over %i steps. [If you think it's your tough luck, just re-run the test]\"%(np.mean(is_dones), len(exp_replay))\n",
    "    \n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_dim\n",
    "    assert act_batch.shape == (10,), \"actions batch should have shape (10,) but is instead %s\"%str(act_batch.shape)\n",
    "    assert reward_batch.shape == (10,), \"rewards batch should have shape (10,) but is instead %s\"%str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (10,), \"is_done batch should have shape (10,) but is instead %s\"%str(is_done_batch.shape)\n",
    "    assert [int(i) in (0,1) for i in is_dones], \"is_done should be strictly True or False\"\n",
    "    assert [0 <= a <= n_actions for a in act_batch], \"actions should be within [0, n_actions]\"\n",
    "    \n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "![img](https://s17.postimg.org/x3hcoi5q7/taget_net.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_network = DQNAgent(\"target_network\", state_dim, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weigths_into_target_network(agent, target_network):\n",
    "    \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n",
    "    assigns = []\n",
    "    for w_agent, w_target in zip(agent.weights, target_network.weights):\n",
    "        assigns.append(tf.assign(w_target, w_agent, validate_shape=True))\n",
    "    tf.get_default_session().run(assigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It works!\n"
     ]
    }
   ],
   "source": [
    "load_weigths_into_target_network(agent, target_network) \n",
    "\n",
    "# check that it works\n",
    "sess.run([tf.assert_equal(w, w_target) for w, w_target in zip(agent.weights, target_network.weights)]);\n",
    "print(\"It works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning\n",
    "Here we write a function similar to `agent.update` from tabular q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders that will be fed with exp_replay.sample(batch_size)\n",
    "obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder(tf.int32, shape=[None])\n",
    "rewards_ph = tf.placeholder(tf.float32, shape=[None])\n",
    "next_obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "is_not_done = 1 - is_done_ph\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take q-values for actions agent just took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n",
    "current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'dqn_agent/conv2d_1/kernel:0' shape=(3, 3, 4, 16) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_1/bias:0' shape=(16,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_2/kernel:0' shape=(3, 3, 16, 32) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_2/bias:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_3/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_3/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_1/kernel:0' shape=(3136, 256) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_1/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_2/kernel:0' shape=(256, 4) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_2/bias:0' shape=(4,) dtype=float32_ref>\"] and loss Tensor(\"Mean:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-833fdcef41e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtd_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    405\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'dqn_agent/conv2d_1/kernel:0' shape=(3, 3, 4, 16) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_1/bias:0' shape=(16,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_2/kernel:0' shape=(3, 3, 16, 32) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_2/bias:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_3/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/conv2d_3/bias:0' shape=(64,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_1/kernel:0' shape=(3136, 256) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_1/bias:0' shape=(256,) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_2/kernel:0' shape=(256, 4) dtype=float32_ref>\", \"<tf.Variable 'dqn_agent/dense_2/bias:0' shape=(4,) dtype=float32_ref>\"] and loss Tensor(\"Mean:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "# compute q-values for NEXT states with target network\n",
    "next_qvalues_target = target_network.get_symbolic_qvalues(next_obs_ph) \n",
    "\n",
    "# compute state values by taking max over next_qvalues_target for all actions\n",
    "next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)\n",
    "\n",
    "# compute Q_reference(s,a) as per formula above.\n",
    "reference_qvalues = rewards_ph + gamma*next_state_values_target*is_not_done\n",
    "\n",
    "# Define loss function for sgd.\n",
    "td_loss = (current_action_qvalues - reference_qvalues) ** 2\n",
    "td_loss = tf.reduce_mean(td_loss)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(td_loss, var_list=agent.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splendid!\n"
     ]
    }
   ],
   "source": [
    "for chk_grad in tf.gradients(reference_qvalues, agent.weights):\n",
    "    error_msg = \"Reference q-values should have no gradient w.r.t. agent weights. Make sure you used target_network qvalues! \"\n",
    "    error_msg += \"If you know what you're doing, ignore this assert.\"\n",
    "    assert chk_grad is None or np.allclose(sess.run(chk_grad), sess.run(chk_grad * 0)), error_msg\n",
    "\n",
    "assert tf.gradients(reference_qvalues, is_not_done)[0] is not None, \"make sure you used is_not_done\"\n",
    "assert tf.gradients(reference_qvalues, rewards_ph)[0] is not None, \"make sure you used rewards\"\n",
    "assert tf.gradients(reference_qvalues, next_obs_ph)[0] is not None, \"make sure you used next states\"\n",
    "assert tf.gradients(reference_qvalues, obs_ph)[0] is None, \"reference qvalues shouldn't depend on current observation!\" # ignore if you're certain it's ok\n",
    "print(\"Splendid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, span, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
    "%matplotlib inline\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(1)\n",
    "play_and_record(agent, env, exp_replay, n_steps=1)\n",
    "\n",
    "def sample_batch(exp_replay, batch_size):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
    "    return {\n",
    "        obs_ph:obs_batch, actions_ph:act_batch, rewards_ph:reward_batch, \n",
    "        next_obs_ph:next_obs_batch, is_done_ph:is_done_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer size = 1, epsilon = 0.01000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-87c0d8b4ccd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_t' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in trange(10):\n",
    "    \n",
    "    # play\n",
    "    play_and_record(agent, env, exp_replay, 10)\n",
    "    \n",
    "#     train\n",
    "#     _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
    "#     td_loss_history.append(loss_t)\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    if i % 500 == 0:\n",
    "        load_weigths_into_target_network(agent, target_network)\n",
    "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
    "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(loss_t)\n",
    "        plt.figure(figsize=[12, 4])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer size = 1, epsilon = 0.01000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-134f46dafbac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_t' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "clear_output(True)\n",
    "print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"mean reward per game\")\n",
    "plt.plot(mean_rw_history)\n",
    "plt.grid()\n",
    "\n",
    "assert not np.isnan(loss_t)\n",
    "plt.figure(figsize=[12, 4])\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"TD loss history (moving average)\")\n",
    "plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n",
    "plt.grid()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's good enough for tutorial.\n"
     ]
    }
   ],
   "source": [
    "assert np.mean(mean_rw_history[-10:]) > 10.\n",
    "print(\"That's good enough for tutorial.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5366"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.network.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ How to interpret plots: __\n",
    "\n",
    "\n",
    "This aint no supervised learning so don't expect anything to improve monotonously. \n",
    "* __ TD loss __ is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
    "* __ mean reward__ is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
    " * In basic q-learning implementation it takes 5-10k steps to \"warm up\" agent before it starts to get better.\n",
    "* __ buffer size__ - this one is simple. It should go up and cap at max size.\n",
    "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - __ it means you need to increase epsilon__. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
    "* Also please ignore first 100-200 steps of each plot - they're just oscillations because of the way moving average works.\n",
    "\n",
    "At first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n",
    "\n",
    "__Training will take time.__ A lot of it actually. An optimistic estimate is to say it's gonna start winning (average reward > 10) after 10k steps. \n",
    "\n",
    "But hey, look on the bright side of things:\n",
    "\n",
    "![img](https://s17.postimg.org/hy2v7r8hr/my_bot_is_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "agent.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training\n",
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Tried to reset environment which is not done. While the monitor is active for BreakoutDeterministic-v4, you cannot call reset() unless the episode is over.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-a808dc74be8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-a808dc74be8b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_monitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-3907fc4d76e6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(env, agent, n_games, greedy, t_max)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_games\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for BreakoutDeterministic-v4, you cannot call reset() unless the episode is over."
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env_monitor = gym.wrappers.Monitor(make_env(),directory=\"videos\",force=True)\n",
    "sessions = [evaluate(env_monitor, agent, n_games=1) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.2.948.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "\n",
    "If you want to play with DQN a bit more, here's a list of things you can try with it:\n",
    "\n",
    "### Easy:\n",
    "* Implementing __double q-learning__ shouldn't be a problem if you've already have target networks in place.\n",
    "  * You will probably need `tf.argmax` to select best actions\n",
    "  * Here's an original [article](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "* __Dueling__ architecture is also quite straightforward if you have standard DQN.\n",
    "  * You will need to change network architecture, namely the q-values layer\n",
    "  * It must now contain two heads: V(s) and A(s,a), both dense layers\n",
    "  * You should then add them up via elemwise sum layer.\n",
    "  * Here's an [article](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard: Prioritized experience replay\n",
    "\n",
    "In this section, you're invited to implement prioritized experience replay\n",
    "\n",
    "* You will probably need to provide a custom data structure\n",
    "* Once pool.update is called, collect the pool.experience_replay.observations, actions, rewards and is_alive and store them in your data structure\n",
    "* You can now sample such transitions in proportion to the error (see [article](https://arxiv.org/abs/1511.05952)) for training.\n",
    "\n",
    "It's probably more convenient to explicitly declare inputs for \"sample observations\", \"sample actions\" and so on to plug them into q-learning.\n",
    "\n",
    "Prioritized (and even normal) experience replay should greatly reduce amount of game sessions you need to play in order to achieve good performance. \n",
    "\n",
    "While it's effect on runtime is limited for atari, more complicated envs (further in the course) will certainly benefit for it.\n",
    "\n",
    "There is even more out there - see this [overview article](https://arxiv.org/abs/1710.02298)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-90121de92c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msubmit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubmit_breakout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msubmit_breakout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ak23@iitbbs.ac.in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BcJkG25r33OQvPff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/abhikcr/New Volume/Self Coaching/Advanced ML/practical_rl/Practical_RL-coursera/week4_approx/submit.py\u001b[0m in \u001b[0;36msubmit_breakout\u001b[0;34m(agent, env, evaluate, email, token)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubmit_breakout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mgrader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WTOZHCn1EeiNwAoZNi-Hrg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VFM7Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11.57\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/abhikcr/New Volume/Self Coaching/Advanced ML/practical_rl/Practical_RL-coursera/week4_approx/submit.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubmit_breakout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mgrader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WTOZHCn1EeiNwAoZNi-Hrg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VFM7Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11.57\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-3907fc4d76e6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(env, agent, n_games, greedy, t_max)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/abhikcr/New Volume/Self Coaching/Advanced ML/practical_rl/Practical_RL-coursera/week4_approx/framebuffer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m\"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mnew_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframebuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from submit import submit_breakout\n",
    "env = make_env()\n",
    "submit_breakout(agent, env, evaluate, 'ak23@iitbbs.ac.in', 'BcJkG25r33OQvPff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[evaluate(env, agent, n_games=1, t_max=100) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
